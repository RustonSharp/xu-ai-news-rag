
import warnings
# æŠ‘åˆ¶huggingface_hubçš„resume_downloadå¼ƒç”¨è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning, module="huggingface_hub")

# å°è¯•åŠ è½½dotenvä»¥æ”¯æŒä».envæ–‡ä»¶ä¸­è¯»å–ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("æç¤ºï¼šæœªå®‰è£…python-dotenvåº“ï¼Œç¯å¢ƒå˜é‡å°†ä»ç³»ç»Ÿä¸­è¯»å–ã€‚")
    print("å¦‚æœéœ€è¦ä».envæ–‡ä»¶ä¸­åŠ è½½ç¯å¢ƒå˜é‡ï¼Œè¯·è¿è¡Œï¼špip install python-dotenv")

from langchain_core.tools import StructuredTool
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import CrossEncoder
import os
import sys
import getpass  
import requests
import json
from langchain_core.tools import Tool
from utils.logging_config import app_logger 
from models.document import Document
import numpy as np
import re
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import silhouette_score
from collections import Counter


# åœ¨çº¿æœç´¢å·¥å…·
def create_online_search_tool():
    """åˆ›å»ºä¸€ä¸ªå¯ä»¥è¿›è¡Œåœ¨çº¿æœç´¢çš„å·¥å…·"""
    # å°è¯•è·å–TAVILY APIå¯†é’¥
    tavily_api_key = os.environ.get('TAVILY_API_KEY')
    
    # å¦‚æœæ²¡æœ‰APIå¯†é’¥ï¼Œæç¤ºç”¨æˆ·è¾“å…¥
    if not tavily_api_key:
        print("æç¤ºï¼šTAVILY_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œå°†è¯·æ±‚ç”¨æˆ·è¾“å…¥")
        try:
            # ä½¿ç”¨getpasså®‰å…¨è·å–APIå¯†é’¥
            tavily_api_key = getpass.getpass("è¯·è¾“å…¥Tavily APIå¯†é’¥ï¼š")
            os.environ["TAVILY_API_KEY"] = tavily_api_key
        except Exception as e:
            print(f"è·å–APIå¯†é’¥å¤±è´¥ï¼š{str(e)}")
            print("è­¦å‘Šï¼šå°†ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢åŠŸèƒ½")
            
            # åˆ›å»ºæ¨¡æ‹Ÿæœç´¢å‡½æ•°
            def mock_search(query):
                return [
                    {"content": f"è¿™æ˜¯å…³äº'{query}'çš„æ¨¡æ‹Ÿæœç´¢ç»“æœã€‚", "url": "https://example.com"}
                ]
            
            # åˆ›å»ºæ¨¡æ‹Ÿæœç´¢å·¥å…·
            return Tool(
                name="OnlineSearch",
                func=mock_search,
                description="ç”¨äºæœç´¢æœ€æ–°çš„ç½‘ç»œä¿¡æ¯ï¼Œå½“ä½ éœ€è¦æœ€æ–°çš„ã€å®æ—¶çš„æˆ–è€…çŸ¥è¯†åº“ä¸­æ²¡æœ‰çš„ä¿¡æ¯æ—¶ä½¿ç”¨æ­¤å·¥å…·"
            )
    
    try:
        print("å·²æˆåŠŸé…ç½®Tavilyåœ¨çº¿æœç´¢åŠŸèƒ½")
        
        # å®šä¹‰æœç´¢åŒ…è£…å‡½æ•°ï¼Œä½¿ç”¨requestsç›´æ¥è°ƒç”¨Tavily API
        def search_wrapper(query):
            try:
                app_logger.info(f"ğŸŒ å¼€å§‹åœ¨çº¿æœç´¢: '{query}'")
                
                # Tavily APIçš„åŸºç¡€URL
                url = "https://api.tavily.com/search"
                app_logger.info(f"ğŸ”— ç›®æ ‡API: {url}")
                
                # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦ä¸ºç©º
                if not tavily_api_key:
                    app_logger.error("âŒ TAVILY_API_KEYæœªè®¾ç½®")
                    return "é”™è¯¯ï¼šTAVILY_API_KEYæœªè®¾ç½®ï¼Œè¯·ç¡®ä¿ç¯å¢ƒå˜é‡å·²æ­£ç¡®é…ç½®æˆ–é€šè¿‡æç¤ºè¾“å…¥APIå¯†é’¥ã€‚"
                
                # å‡†å¤‡è¯·æ±‚å‚æ•°
                payload = {
                    "api_key": tavily_api_key,
                    "query": query,
                    "search_depth": "basic",  # åŸºç¡€æœç´¢æ·±åº¦
                    "max_results": 3,  # è¿”å›3ä¸ªç»“æœ
                    "include_answer": False,  # ä¸åŒ…å«ç›´æ¥å›ç­”
                    "include_raw_content": False,  # ä¸åŒ…å«åŸå§‹å†…å®¹
                    "include_images": False  # ä¸åŒ…å«å›¾ç‰‡
                }
                app_logger.info(f"ğŸ“‹ æœç´¢å‚æ•°: {payload}")
                
                # å‘é€POSTè¯·æ±‚
                app_logger.info("ğŸš€ æ­£åœ¨å‘é€æœç´¢è¯·æ±‚...")
                response = requests.post(url, json=payload)
                app_logger.info(f"ğŸ“¡ æ”¶åˆ°å“åº”ï¼ŒçŠ¶æ€ç : {response.status_code}")
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code != 200:
                    app_logger.error(f"âŒ æœç´¢è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    # æ›´å®‰å…¨åœ°æ˜¾ç¤ºAPIå¯†é’¥ä¿¡æ¯ï¼Œé¿å…æ³„éœ²å®Œæ•´å¯†é’¥
                    if len(tavily_api_key) > 10:
                        api_key_display = f"{tavily_api_key[:5]}...{tavily_api_key[-5:]}"
                    else:
                        api_key_display = "å¯†é’¥é•¿åº¦ä¸è¶³ï¼Œæ— æ³•å®‰å…¨æ˜¾ç¤º"
                    app_logger.error(f"ğŸ”‘ APIå¯†é’¥æ£€æŸ¥: {api_key_display}")
                    return f"æœç´¢è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{response.text}ï¼ŒAPIå¯†é’¥æ£€æŸ¥ï¼š{api_key_display}"
                
                # è§£æå“åº”JSON
                try:
                    app_logger.info("ğŸ“„ æ­£åœ¨è§£ææœç´¢ç»“æœ...")
                    results = response.json()
                    app_logger.info("âœ… JSONè§£ææˆåŠŸ")
                except json.JSONDecodeError as e:
                    app_logger.error(f"âŒ JSONè§£æå¤±è´¥: {str(e)}")
                    return f"æ— æ³•è§£ææœç´¢ç»“æœï¼š{response.text}"
                
                # æå–ç»“æœåˆ—è¡¨
                if "results" not in results:
                    app_logger.error("âŒ æœç´¢ç»“æœæ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘'results'å­—æ®µ")
                    return f"æœç´¢ç»“æœæ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘'results'å­—æ®µ"
                
                app_logger.info(f"ğŸ“Š æ‰¾åˆ° {len(results['results'])} ä¸ªæœç´¢ç»“æœ")
                
                # æ ¼å¼åŒ–ç»“æœ
                app_logger.info("ğŸ“‹ å¼€å§‹æ ¼å¼åŒ–æœç´¢ç»“æœ...")
                formatted_results = []
                for i, result in enumerate(results["results"], 1):
                    formatted_result = {
                        "content": result.get("content", ""),
                        "url": result.get("url", ""),
                        "title": result.get("title", "")
                    }
                    formatted_results.append(formatted_result)
                    app_logger.info(f"ğŸ“„ ç»“æœ {i}: {formatted_result['title'][:50]}...")
                
                app_logger.info(f"ğŸ¯ åœ¨çº¿æœç´¢å®Œæˆï¼Œè¿”å› {len(formatted_results)} ä¸ªæ ¼å¼åŒ–ç»“æœ")
                return formatted_results
            except requests.RequestException as e:
                app_logger.error(f"âŒ æœç´¢è¯·æ±‚å‡ºé”™: {str(e)}")
                return f"æœç´¢è¯·æ±‚å‡ºé”™ï¼š{str(e)}"
            except Exception as e:
                app_logger.error(f"âŒ æœç´¢å‡ºé”™: {str(e)}")
                return f"æœç´¢å‡ºé”™ï¼š{str(e)}ã€‚è¯·æ£€æŸ¥TAVILY_API_KEYæ˜¯å¦æ­£ç¡®è®¾ç½®ã€‚"
        
        # åˆ›å»ºåœ¨çº¿æœç´¢å·¥å…·
        online_search_tool = Tool(
            name="OnlineSearch",
            func=search_wrapper,
            description="ç”¨äºæœç´¢æœ€æ–°çš„ç½‘ç»œä¿¡æ¯ï¼Œå½“ä½ éœ€è¦æœ€æ–°çš„ã€å®æ—¶çš„æˆ–è€…çŸ¥è¯†åº“ä¸­æ²¡æœ‰çš„ä¿¡æ¯æ—¶ä½¿ç”¨æ­¤å·¥å…·"
        )
        
        return online_search_tool
    except Exception as e:
        print(f"åˆ›å»ºTavilyæœç´¢å·¥å…·å¤±è´¥ï¼š{str(e)}")
        print("è­¦å‘Šï¼šå°†ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢åŠŸèƒ½")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæœç´¢å‡½æ•°
        def mock_search(query):
            return [
                {"content": f"è¿™æ˜¯å…³äº'{query}'çš„æ¨¡æ‹Ÿæœç´¢ç»“æœã€‚", "url": "https://example.com"}
            ]
        
        # åˆ›å»ºæ¨¡æ‹Ÿæœç´¢å·¥å…·
        return Tool(
            name="OnlineSearch",
            func=mock_search,
            description="ç”¨äºæœç´¢æœ€æ–°çš„ç½‘ç»œä¿¡æ¯ï¼Œå½“ä½ éœ€è¦æœ€æ–°çš„ã€å®æ—¶çš„æˆ–è€…çŸ¥è¯†åº“ä¸­æ²¡æœ‰çš„ä¿¡æ¯æ—¶ä½¿ç”¨æ­¤å·¥å…·"
        )
def create_online_search_tool_v2():
    """åˆ›å»ºä¸€ä¸ªå¯ä»¥è¿›è¡Œåœ¨çº¿æœç´¢çš„å·¥å…·"""
    # å°è¯•è·å–TAVILY APIå¯†é’¥
    tavily_api_key = os.environ.get('TAVILY_API_KEY')
    
    # å¦‚æœæ²¡æœ‰APIå¯†é’¥ï¼Œæç¤ºç”¨æˆ·è¾“å…¥
    if not tavily_api_key:
        print("æç¤ºï¼šTAVILY_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®clearï¼Œå°†è¯·æ±‚ç”¨æˆ·è¾“å…¥")
        try:
            # ä½¿ç”¨getpasså®‰å…¨è·å–APIå¯†é’¥
            tavily_api_key = getpass.getpass("è¯·è¾“å…¥Tavily APIå¯†é’¥ï¼š")
            os.environ["TAVILY_API_KEY"] = tavily_api_key
        except Exception as e:
            print(f"è·å–APIå¯†é’¥å¤±è´¥ï¼š{str(e)}")
            print("è­¦å‘Šï¼šå°†ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢åŠŸèƒ½")
            
            # åˆ›å»ºæ¨¡æ‹Ÿæœç´¢å‡½æ•°
            def mock_search(query):
                return [
                    {"content": f"è¿™æ˜¯å…³äº'{query}'çš„æ¨¡æ‹Ÿæœç´¢ç»“æœã€‚", "url": "https://example.com"}
                ]
            
            # åˆ›å»ºæ¨¡æ‹Ÿæœç´¢å·¥å…·
            return Tool(
                name="OnlineSearchV2",
                func=mock_search,
                description="ä½¿ç”¨æ— å¤´æµè§ˆå™¨è®¿é—®Bingæœç´¢ç½‘ç«™ï¼Œç”¨äºæœç´¢æœ€æ–°çš„ç½‘ç»œä¿¡æ¯ï¼Œå½“ä½ éœ€è¦æœ€æ–°çš„ã€å®æ—¶çš„æˆ–è€…çŸ¥è¯†åº“ä¸­æ²¡æœ‰çš„ä¿¡æ¯æ—¶ä½¿ç”¨æ­¤å·¥å…·"
            )
    
    try:
        print("å·²æˆåŠŸé…ç½®Tavilyåœ¨çº¿æœç´¢åŠŸèƒ½")
        
        # å®šä¹‰æœç´¢åŒ…è£…å‡½æ•°ï¼Œä½¿ç”¨requestsç›´æ¥è°ƒç”¨Tavily API
        def search_wrapper(query):
            try:
                # Tavily APIçš„åŸºç¡€URL
                url = "https://api.tavily.com/search"
                
                # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦ä¸ºç©º
                if not tavily_api_key:
                    return "é”™è¯¯ï¼šTAVILY_API_KEYæœªè®¾ç½®ï¼Œè¯·ç¡®ä¿ç¯å¢ƒå˜é‡å·²æ­£ç¡®é…ç½®æˆ–é€šè¿‡æç¤ºè¾“å…¥APIå¯†é’¥ã€‚"
                
                # å‡†å¤‡è¯·æ±‚å‚æ•°
                payload = {
                    "api_key": tavily_api_key,
                    "query": query,
                    "search_depth": "basic",  # åŸºç¡€æœç´¢æ·±åº¦
                    "max_results": 5,  # è¿”å›5ä¸ªç»“æœ
                    "include_answer": False,  # ä¸åŒ…å«ç›´æ¥å›ç­”
                    "include_raw_content": False,  # ä¸åŒ…å«åŸå§‹å†…å®¹
                    "include_images": False  # ä¸åŒ…å«å›¾ç‰‡
                }
                
                # å‘é€POSTè¯·æ±‚
                response = requests.post(url, json=payload)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code != 200:
                    # æ›´å®‰å…¨åœ°æ˜¾ç¤ºAPIå¯†é’¥ä¿¡æ¯ï¼Œé¿å…æ³„éœ²å®Œæ•´å¯†é’¥
                    if len(tavily_api_key) > 10:
                        api_key_display = f"{tavily_api_key[:5]}...{tavily_api_key[-5:]}"
                    else:
                        api_key_display = "å¯†é’¥é•¿åº¦ä¸è¶³ï¼Œæ— æ³•å®‰å…¨æ˜¾ç¤º"
                    return f"æœç´¢è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{response.text}ï¼ŒAPIå¯†é’¥æ£€æŸ¥ï¼š{api_key_display}"
                
                # è§£æå“åº”JSON
                try:
                    results = response.json()
                except json.JSONDecodeError:
                    return f"æ— æ³•è§£ææœç´¢ç»“æœï¼š{response.text}"
                
                # æå–ç»“æœåˆ—è¡¨
                if "results" not in results:
                    return f"æœç´¢ç»“æœæ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘'results'å­—æ®µ"
                
                # æ ¼å¼åŒ–ç»“æœ
                formatted_results = []
                for i, result in enumerate(results["results"]):
                    formatted_results.append({
                        "content": result.get("content", ""),
                        "url": result.get("url", ""),
                        "title": result.get("title", ""),
                        "published_date": "",
                        "score": 1.0 - (i * 0.1),  # ç®€å•è¯„åˆ†ï¼Œç»“æœè¶Šé å‰åˆ†æ•°è¶Šé«˜
                        "is_direct_answer": False
                    })
                
                return formatted_results
            except requests.RequestException as e:
                return f"æœç´¢è¯·æ±‚å‡ºé”™ï¼š{str(e)}"
            except Exception as e:
                return f"æœç´¢å‡ºé”™ï¼š{str(e)}ã€‚è¯·æ£€æŸ¥TAVILY_API_KEYæ˜¯å¦æ­£ç¡®è®¾ç½®ã€‚"
        
        # åˆ›å»ºåœ¨çº¿æœç´¢å·¥å…·
        online_search_tool = Tool(
            name="OnlineSearchV2",
            func=search_wrapper,
            description="ä½¿ç”¨Tavily APIè¿›è¡Œæœç´¢ï¼Œç”¨äºæœç´¢æœ€æ–°çš„ç½‘ç»œä¿¡æ¯ï¼Œå½“ä½ éœ€è¦æœ€æ–°çš„ã€å®æ—¶çš„æˆ–è€…çŸ¥è¯†åº“ä¸­æ²¡æœ‰çš„ä¿¡æ¯æ—¶ä½¿ç”¨æ­¤å·¥å…·"
        )
        
        return online_search_tool
    except Exception as e:
        print(f"åˆ›å»ºTavilyæœç´¢å·¥å…·å¤±è´¥ï¼š{str(e)}")
        print("è­¦å‘Šï¼šå°†ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢åŠŸèƒ½")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæœç´¢å‡½æ•°
        def mock_search(query):
            return [
                {"content": f"è¿™æ˜¯å…³äº'{query}'çš„æ¨¡æ‹Ÿæœç´¢ç»“æœã€‚", "url": "https://example.com"}
            ]
        
        # åˆ›å»ºæ¨¡æ‹Ÿæœç´¢å·¥å…·
        return Tool(
            name="OnlineSearchV2",
            func=mock_search,
            description="ä½¿ç”¨æ— å¤´æµè§ˆå™¨è®¿é—®Bingæœç´¢ç½‘ç«™ï¼Œç”¨äºæœç´¢æœ€æ–°çš„ç½‘ç»œä¿¡æ¯ï¼Œå½“ä½ éœ€è¦æœ€æ–°çš„ã€å®æ—¶çš„æˆ–è€…çŸ¥è¯†åº“ä¸­æ²¡æœ‰çš„ä¿¡æ¯æ—¶ä½¿ç”¨æ­¤å·¥å…·"
        )

# çŸ¥è¯†åº“å·¥å…·ï¼ˆå‘é‡æ•°æ®åº“ï¼‰
def create_knowledge_base_tool():
    """åˆ›å»ºä¸€ä¸ªå¯ä»¥å°†æ•°æ®å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“å¹¶è¿›è¡Œæ£€ç´¢çš„å·¥å…·"""
    # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹åç§°
    embedding_model_name = os.getenv("EMBEDDING_MODEL_NAME")
    rerank_model_name = os.getenv("RERANK_MODEL_NAME")
    
    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - æ·»åŠ æ›´å¤šå¤‡ç”¨é€‰é¡¹å’Œæœ¬åœ°æ¨¡å‹æ”¯æŒ
    # æ³¨æ„ï¼šå½“å‰ç‰ˆæœ¬çš„langchain_huggingfaceå¯èƒ½ä¼šæ˜¾ç¤ºFutureWarning: `resume_download` is deprecated
    # è¿™æ˜¯ä¸€ä¸ªè­¦å‘Šï¼Œä¸å½±å“åŠŸèƒ½æ­£å¸¸è¿è¡Œ
    embedding_models = [
        "sentence-transformers/all-MiniLM-L6-v2",
        "paraphrase-multilingual-MiniLM-L12-v2", 
        "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        "sentence-transformers/distiluse-base-multilingual-cased"
    ]
    
    embeddings = None
    for model_name in embedding_models:
        try:
            print(f"å°è¯•åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
            embeddings = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={'device': 'cpu'},
                cache_folder="./models_cache"  # ä½¿ç”¨cacheç›®å½•å­˜æ”¾æ¨¡å‹
            )
            print(f"æˆåŠŸåŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
            break
        except Exception as e:
            app_logger.warning(f"æ— æ³•åŠ è½½åµŒå…¥æ¨¡å‹ {model_name}: {str(e)}")
            print(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½åµŒå…¥æ¨¡å‹ {model_name}ã€‚é”™è¯¯ä¿¡æ¯ï¼š{str(e)}")
            continue
    
    if embeddings is None:
        app_logger.error("æ‰€æœ‰åµŒå…¥æ¨¡å‹éƒ½æ— æ³•åŠ è½½")
        print("é”™è¯¯ï¼šæ‰€æœ‰åµŒå…¥æ¨¡å‹éƒ½æ— æ³•åŠ è½½ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å‹ã€‚")
        print("å»ºè®®ï¼š")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("2. å°è¯•ä½¿ç”¨VPN")
        print("3. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°")
        raise Exception("æ— æ³•åŠ è½½ä»»ä½•åµŒå…¥æ¨¡å‹")
    
    # åˆå§‹åŒ–é‡æ’æ¨¡å‹ - ä½¿ç”¨å…¬å¼€å¯ç”¨çš„æ¨¡å‹
    try:
        reranker = CrossEncoder(rerank_model_name, device='cpu')
    except Exception as e:
        app_logger.warning(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½é‡æ’æ¨¡å‹ {rerank_model_name}ï¼Œé”™è¯¯ï¼š{str(e)}")
        app_logger.info("å°†ä½¿ç”¨ç®€å•ç›¸ä¼¼åº¦æ£€ç´¢ï¼Œä¸è¿›è¡Œé‡æ’")
        # è®¾ç½®ä¸€ä¸ªæ ‡å¿—ï¼Œè¡¨ç¤ºé‡æ’æ¨¡å‹ä¸å¯ç”¨
        reranker = None
    
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    # å¦‚æœå‘é‡æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºæ–°çš„
    # ä»ç¯å¢ƒå˜é‡è·å–è·¯å¾„ï¼Œæ•°æ®å­˜å‚¨åœ¨backend/dataç›®å½•ä¸‹
    faiss_index_path = os.getenv("FAISS_INDEX_PATH", "./data/index.faiss")
    # faiss_index_path = os.getenv("FAISS_INDEX_PATH")
    # ä»FAISS_INDEX_PATHæå–ç›®å½•è·¯å¾„
    vectorstore_path = os.path.dirname(faiss_index_path)
    index_faiss_path = faiss_index_path
    
    # ç¡®ä¿vectorstore_faissç›®å½•å­˜åœ¨
    if not os.path.exists(vectorstore_path):
        os.makedirs(vectorstore_path)
    
    # æ£€æŸ¥index.faissæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if os.path.exists(index_faiss_path):
        try:
            vectorstore = FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)
        except Exception as e:
            app_logger.warning(f"è­¦å‘Šï¼šåŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°çš„æ•°æ®åº“ã€‚é”™è¯¯ï¼š{str(e)}")
            # åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
            vectorstore = FAISS.from_texts(["Placeholder text"], embeddings)
            vectorstore.save_local(vectorstore_path)
    else:
        # åˆ›å»ºç©ºçš„å‘é‡æ•°æ®åº“
        vectorstore = FAISS.from_texts(["Placeholder text"], embeddings)
        vectorstore.save_local(vectorstore_path)
    
    # æ–‡æ¡£å¤„ç†å‡½æ•°
    def process_and_store_documents(documents: list):
        """å¤„ç†æ–‡æ¡£å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“"""
        app_logger.info(f"ğŸ“š å¼€å§‹å¤„ç†æ–‡æ¡£å­˜å‚¨: å…± {len(documents)} ä¸ªæ–‡æ¡£")
        
        # åˆ†å‰²æ–‡æ¡£
        app_logger.info("âœ‚ï¸ åˆå§‹åŒ–æ–‡æ¡£åˆ†å‰²å™¨")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        app_logger.info("âœ… æ–‡æ¡£åˆ†å‰²å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åˆ†å‰²æ‰€æœ‰æ–‡æ¡£å¹¶æ·»åŠ å…ƒæ•°æ®
        app_logger.info("ğŸ“ å¼€å§‹åˆ†å‰²æ–‡æ¡£å¹¶æå–å…ƒæ•°æ®")
        all_chunks = []
        all_metadatas = []
        for i, doc in enumerate(documents):
            app_logger.info(f"ğŸ“„ å¤„ç†æ–‡æ¡£ {i+1}/{len(documents)}")
            
            # æ£€æŸ¥æ–‡æ¡£ç±»å‹å¹¶æå–å†…å®¹
            if isinstance(doc, dict):
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œæå–ç›¸å…³å­—æ®µ
                content = doc.get("description", "")
                title = doc.get("title", f"Document_{i}")
                tags = doc.get("tags", "")
                pub_date = doc.get("pub_date", "")
                author = doc.get("author", "")
                app_logger.info(f"ğŸ“‹ å­—å…¸æ ¼å¼æ–‡æ¡£: {title[:50]}...")
            else:
                # å¦‚æœæ˜¯Documentå¯¹è±¡ï¼Œæå–å±æ€§
                content = doc.description if hasattr(doc, 'description') else str(doc)
                title = doc.title if hasattr(doc, 'title') else f"Document_{i}"
                tags = doc.tags if hasattr(doc, 'tags') else ""
                pub_date = doc.pub_date.isoformat() if hasattr(doc, 'pub_date') and doc.pub_date is not None else ""
                author = doc.author if hasattr(doc, 'author') else ""
                app_logger.info(f"ğŸ“‹ Documentå¯¹è±¡: {title[:50]}...")
            
            # åˆ†å‰²æ–‡æ¡£å†…å®¹
            app_logger.info(f"âœ‚ï¸ åˆ†å‰²æ–‡æ¡£å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            chunks = text_splitter.split_text(content)
            app_logger.info(f"ğŸ“Š åˆ†å‰²å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µ")
            
            # ä¸ºæ¯ä¸ªæ–‡æ¡£ç‰‡æ®µæ·»åŠ å…ƒæ•°æ®
            metadatas = [{
                "source": f"document_{i}", 
                "chunk": j, 
                "title": title,
                "tags": tags,
                "pub_date": pub_date,
                "author": author,
            } for j in range(len(chunks))]
            all_chunks.extend(chunks)
            all_metadatas.extend(metadatas)
            app_logger.info(f"âœ… æ–‡æ¡£ {i+1} å¤„ç†å®Œæˆ")
        
        # å¦‚æœå‘é‡æ•°æ®åº“ä¸­åªæœ‰å ä½ç¬¦æ–‡æœ¬ï¼Œåˆ™åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
        app_logger.info("ğŸ” æ£€æŸ¥å‘é‡æ•°æ®åº“çŠ¶æ€...")
        try:
            # è·å–ç¬¬ä¸€ä¸ªæ–‡æ¡£ID
            first_doc_id = list(vectorstore.index_to_docstore_id.values())[0]
            first_doc = vectorstore.docstore.search(first_doc_id)
            is_placeholder = len(vectorstore.index_to_docstore_id) == 1 and "Placeholder text" in str(first_doc)
            app_logger.info(f"ğŸ“Š å‘é‡æ•°æ®åº“çŠ¶æ€: å ä½ç¬¦={is_placeholder}, æ–‡æ¡£æ•°={len(vectorstore.index_to_docstore_id)}")
        except (IndexError, KeyError, AttributeError) as e:
            # å¦‚æœè·å–ç¬¬ä¸€ä¸ªæ–‡æ¡£æ—¶å‡ºé”™ï¼Œå‡è®¾ä¸æ˜¯å ä½ç¬¦
            app_logger.warning(f"âš ï¸ æ£€æŸ¥å ä½ç¬¦æ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
            is_placeholder = False
            
        if is_placeholder:
            app_logger.info("ğŸ”„ æ£€æµ‹åˆ°å ä½ç¬¦æ–‡æœ¬ï¼Œåˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“")
            # åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“ï¼Œä¸åŒ…å«å ä½ç¬¦æ–‡æœ¬
            new_vectorstore = FAISS.from_texts(all_chunks, embeddings, metadatas=all_metadatas)
            # æ›¿æ¢åŸæ¥çš„å‘é‡æ•°æ®åº“
            vectorstore.index_to_docstore_id = new_vectorstore.index_to_docstore_id
            vectorstore.docstore = new_vectorstore.docstore
            vectorstore.index = new_vectorstore.index
            app_logger.info("âœ… æ–°å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆ")
        else:
            app_logger.info("â• å‘ç°æœ‰å‘é‡æ•°æ®åº“æ·»åŠ æ–‡æ¡£")
            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
            vectorstore.add_texts(all_chunks, all_metadatas)
            app_logger.info("âœ… æ–‡æ¡£æ·»åŠ å®Œæˆ")
        
        # ä¿å­˜å‘é‡æ•°æ®åº“åˆ°ç¯å¢ƒå˜é‡æŒ‡å®šçš„è·¯å¾„
        app_logger.info(f"ğŸ’¾ ä¿å­˜å‘é‡æ•°æ®åº“åˆ°: {vectorstore_path}")
        vectorstore.save_local(vectorstore_path)
        app_logger.info("âœ… å‘é‡æ•°æ®åº“ä¿å­˜å®Œæˆ")
        
        result_message = f"æˆåŠŸå¤„ç†å¹¶å­˜å‚¨äº†{len(all_chunks)}ä¸ªæ–‡æ¡£ç‰‡æ®µåˆ°å‘é‡æ•°æ®åº“"
        app_logger.info(f"ğŸ¯ æ–‡æ¡£å­˜å‚¨å®Œæˆ: {result_message}")
        return result_message
    
    # æ£€ç´¢å‡½æ•°
    def retrieve_from_knowledge_base(query, k=3, rerank=True):
        """ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£"""
        app_logger.info(f"ğŸ” å¼€å§‹ä»å‘é‡æ•°æ®åº“æ£€ç´¢: '{query}'")
        app_logger.info(f"ğŸ“Š æ£€ç´¢å‚æ•°: k={k}, rerank={rerank}")
        
        # è·å–åˆå§‹æ£€ç´¢ç»“æœ
        if rerank and reranker is not None:
            # ä¸ºäº†é‡æ’éœ€è¦è·å–æ›´å¤šç»“æœ
            initial_k = k * 3
            app_logger.info(f"ğŸ”„ å¯ç”¨é‡æ’æ¨¡å¼ï¼Œåˆå§‹æ£€ç´¢æ•°é‡: {initial_k}")
        else:
            initial_k = k
            # å¦‚æœæ²¡æœ‰é‡æ’æ¨¡å‹æˆ–ä¸è¿›è¡Œé‡æ’ï¼Œåˆ™ä¸è¿›è¡Œé¢å¤–å¤„ç†
            rerank = False
            app_logger.info(f"âš¡ ç›´æ¥æ£€ç´¢æ¨¡å¼ï¼Œæ£€ç´¢æ•°é‡: {initial_k}")
            
        app_logger.info("ğŸ“š æ­£åœ¨æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢...")
        results = vectorstore.similarity_search(query, k=initial_k)
        app_logger.info(f"âœ… å‘é‡æœç´¢å®Œæˆï¼Œè·å¾— {len(results)} ä¸ªåˆå§‹ç»“æœ")
        
        # ä½¿ç”¨é‡æ’æ¨¡å‹å¯¹ç»“æœè¿›è¡Œæ’åºï¼ˆä»…åœ¨æ¨¡å‹å¯ç”¨æ—¶ï¼‰
        if rerank and results and reranker is not None:
            try:
                app_logger.info("ğŸ”„ å¼€å§‹é‡æ’æ¨¡å‹å¤„ç†...")
                # å‡†å¤‡ç”¨äºé‡æ’çš„æ–‡æœ¬å¯¹
                sentence_pairs = [[query, result.page_content] for result in results]
                app_logger.info(f"ğŸ“ å‡†å¤‡é‡æ’æ–‡æœ¬å¯¹ï¼Œå…± {len(sentence_pairs)} å¯¹")
                
                # è·å–é‡æ’åˆ†æ•°
                app_logger.info("ğŸ§  æ­£åœ¨è®¡ç®—é‡æ’åˆ†æ•°...")
                scores = reranker.predict(sentence_pairs)
                app_logger.info(f"ğŸ“Š é‡æ’åˆ†æ•°è®¡ç®—å®Œæˆ: {scores}")
                
                # æŒ‰åˆ†æ•°é™åºæ’åº
                results_with_scores = list(zip(results, scores))
                results_with_scores.sort(key=lambda x: x[1], reverse=True)
                app_logger.info("ğŸ”¢ æŒ‰é‡æ’åˆ†æ•°æ’åºå®Œæˆ")
                
                # åªä¿ç•™å‰kä¸ªç»“æœ
                results = [result for result, _ in results_with_scores[:k]]
                app_logger.info(f"âœ… é‡æ’å®Œæˆï¼Œä¿ç•™å‰ {k} ä¸ªæœ€ç›¸å…³ç»“æœ")
            except Exception as e:
                app_logger.warning(f"âš ï¸ é‡æ’è¿‡ç¨‹å‡ºé”™ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœã€‚é”™è¯¯ï¼š{str(e)}")
                # ç¡®ä¿resultsä»ç„¶æ˜¯å¯è¿­ä»£çš„ï¼Œå¹¶ä¸”ä¿æŒåŸå§‹ç»“æœ
                if not isinstance(results, list):
                    # å¦‚æœresultsä¸æ˜¯åˆ—è¡¨ï¼Œå°è¯•ä»vectorstoreé‡æ–°è·å–
                    try:
                        app_logger.info("ğŸ”„ å°è¯•é‡æ–°è·å–æ£€ç´¢ç»“æœ...")
                        results = vectorstore.similarity_search(query, k=k)
                        app_logger.info(f"âœ… é‡æ–°è·å–æˆåŠŸï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
                    except Exception as e2:
                        app_logger.error(f"âŒ é‡æ–°è·å–å¤±è´¥: {str(e2)}")
                        results = []
                # å¦‚æœresultsä»ç„¶ä¸æ˜¯åˆ—è¡¨ï¼Œè®¾ç½®ä¸ºç©ºåˆ—è¡¨
                if not isinstance(results, list):
                    results = []
        else:
            # å¦‚æœæ²¡æœ‰é‡æ’æˆ–é‡æ’æ¨¡å‹ä¸å¯ç”¨ï¼Œç¡®ä¿resultsæ˜¯åˆ—è¡¨
            if not isinstance(results, list):
                results = []
            app_logger.info("âš¡ è·³è¿‡é‡æ’ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
        
        # ç¡®ä¿resultsæ˜¯åˆ—è¡¨å¹¶ä¸”æœ‰å†…å®¹
        if not isinstance(results, list):
            results = []
        elif len(results) == 0:
            # å¦‚æœresultsä¸ºç©ºï¼Œå°è¯•é‡æ–°è·å–
            try:
                app_logger.warning("âš ï¸ æ£€ç´¢ç»“æœä¸ºç©ºï¼Œå°è¯•é‡æ–°è·å–...")
                results = vectorstore.similarity_search(query, k=k)
                app_logger.info(f"âœ… é‡æ–°è·å–æˆåŠŸï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
            except Exception as e:
                app_logger.error(f"âŒ é‡æ–°è·å–å¤±è´¥: {str(e)}")
                results = []
        
        # æ ¼å¼åŒ–ç»“æœ
        app_logger.info("ğŸ“‹ å¼€å§‹æ ¼å¼åŒ–æ£€ç´¢ç»“æœ...")
        formatted_results = []
        for i, result in enumerate(results, 1):
            formatted_results.append({
                "id": i,
                "content": result.page_content,
                "metadata": result.metadata
            })
        
        app_logger.info(f"ğŸ¯ å‘é‡æ•°æ®åº“æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(formatted_results)} ä¸ªæ ¼å¼åŒ–ç»“æœ")
        return formatted_results
    
    # åˆ›å»ºå·¥å…· - è¿›ä¸€æ­¥è°ƒæ•´å‡½æ•°å®ç°ä»¥å¤„ç†ä¸åŒçš„æ•°æ®ç±»å‹
    def knowledge_base_func(action: str, documents: list = None, query=None, k: int = 3, rerank: bool = True):
        """å¤„ç†çŸ¥è¯†åº“å·¥å…·çš„è¾“å…¥æ•°æ®
        
        Args:
            action: æ“ä½œç±»å‹ï¼Œæ”¯æŒ'store'å’Œ'retrieve'
            documents: å¯¹äº'store'æ“ä½œï¼Œæ˜¯Documentå¯¹è±¡åˆ—è¡¨æˆ–æ–‡æ¡£å­—å…¸åˆ—è¡¨
            query: å¯¹äº'retrieve'æ“ä½œï¼Œæ˜¯æŸ¥è¯¢å­—ç¬¦ä¸²
            k: å¯¹äº'retrieve'æ“ä½œï¼Œè¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤ä¸º3
            rerank: å¯¹äº'retrieve'æ“ä½œï¼Œæ˜¯å¦å¯ç”¨ç»“æœé‡æ’ï¼Œé»˜è®¤ä¸ºTrue
        """
        app_logger.info(f"ğŸ”§ çŸ¥è¯†åº“å·¥å…·è°ƒç”¨: action={action}")
        
        if action == "store":
            app_logger.info(f"ğŸ“š å­˜å‚¨æ“ä½œ: å¤„ç† {len(documents) if documents else 0} ä¸ªæ–‡æ¡£")
            # æ£€æŸ¥documentså‚æ•°æ˜¯å¦ä¸ºDocumentå¯¹è±¡åˆ—è¡¨
            if documents and len(documents) > 0 and hasattr(documents[0], 'title'):
                app_logger.info("ğŸ”„ æ£€æµ‹åˆ°Documentå¯¹è±¡åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼")
                # å¦‚æœæ˜¯Documentå¯¹è±¡åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                from models.document import Document
                document_dicts = []
                for doc in documents:
                    if isinstance(doc, Document):
                        doc_dict = {
                            "title": doc.title,
                            "description": doc.description,
                            "tags": doc.tags,
                            "pub_date": doc.pub_date,
                            "author": doc.author
                        }
                        document_dicts.append(doc_dict)
                    else:
                        # å¦‚æœå·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥æ·»åŠ 
                        document_dicts.append(doc)
                app_logger.info(f"âœ… è½¬æ¢å®Œæˆï¼Œå…± {len(document_dicts)} ä¸ªæ–‡æ¡£")
                return process_and_store_documents(document_dicts)
            else:
                app_logger.info("ğŸ“„ ç›´æ¥å¤„ç†å­—å…¸æ ¼å¼æ–‡æ¡£")
                # å¦‚æœæ˜¯å­—å…¸åˆ—è¡¨ï¼Œç›´æ¥å¤„ç†
                return process_and_store_documents(documents)
        elif action == "retrieve":
            app_logger.info(f"ğŸ” æ£€ç´¢æ“ä½œ: query='{query}', k={k}, rerank={rerank}")
            return retrieve_from_knowledge_base(query, k, rerank)
        elif action == "cluster_analysis":
            app_logger.info("ğŸ“Š èšç±»åˆ†ææ“ä½œ")
            return knowledge_base_cluster_analysis()
        else:
            app_logger.error(f"âŒ ä¸æ”¯æŒçš„æ“ä½œ: {action}")
            return f"ä¸æ”¯æŒçš„æ“ä½œï¼š{action}"
    
    knowledge_base_tool = StructuredTool.from_function(
        func=knowledge_base_func,
        name="KnowledgeBase",
        description="ç”¨äºå¤„ç†å’Œæ£€ç´¢å‘é‡æ•°æ®åº“ä¸­çš„çŸ¥è¯†ï¼Œæ”¯æŒå­˜å‚¨æ–‡æ¡£(store)å’Œæ£€ç´¢ä¿¡æ¯(retrieve)ä¸¤ç§æ“ä½œ"
    )
    
    return knowledge_base_tool

# ç”Ÿæˆèšç±»æ ‡ç­¾çš„å‡½æ•°
def generate_cluster_label(keywords):
    """
    æ ¹æ®å…³é”®è¯ç”Ÿæˆèšç±»çš„æè¿°æ€§æ ‡ç­¾
    
    Args:
        keywords (list): èšç±»çš„å…³é”®è¯åˆ—è¡¨
        
    Returns:
        str: èšç±»çš„æè¿°æ€§æ ‡ç­¾
    """
    if not keywords:
        return "æœªåˆ†ç±»"
    
    # å¸¸è§ä¸»é¢˜å…³é”®è¯æ˜ å°„
    topic_keywords = {
        "æ”¿æ²»": ["æ”¿åºœ", "å›½å®¶", "æ€»ç»Ÿ", "é¢†å¯¼äºº", "é€‰ä¸¾", "æ”¿ç­–", "è®®ä¼š", "å…šæ´¾", "æ”¿æ²»", "å¤–äº¤"],
        "ç»æµ": ["ç»æµ", "é‡‘è", "å¸‚åœº", "è‚¡ç¥¨", "æŠ•èµ„", "è´¸æ˜“", "è´§å¸", "é“¶è¡Œ", "è´¢æ”¿", "GDP"],
        "ç§‘æŠ€": ["ç§‘æŠ€", "æŠ€æœ¯", "äººå·¥æ™ºèƒ½", "æ•°æ®", "äº’è”ç½‘", "è½¯ä»¶", "è®¡ç®—æœº", "æ•°å­—", "åˆ›æ–°", "ç ”å‘"],
        "å†›äº‹": ["å†›äº‹", "å†›é˜Ÿ", "å›½é˜²", "æ­¦å™¨", "æˆ˜äº‰", "å†²çª", "å®‰å…¨", "æ¼”ä¹ ", "åŸºåœ°", "æ­¦è£…"],
        "ç¤¾ä¼š": ["ç¤¾ä¼š", "æ–‡åŒ–", "æ•™è‚²", "å¥åº·", "åŒ»ç–—", "ç¯å¢ƒ", "æ°”å€™", "äººå£", "ç¤¾åŒº", "æ°‘ç”Ÿ"],
        "ä½“è‚²": ["ä½“è‚²", "è¶³çƒ", "ç¯®çƒ", "æ¯”èµ›", "è¿åŠ¨å‘˜", "å¥¥è¿", "ä¸–ç•Œæ¯", "å† å†›", "è”èµ›", "æ•™ç»ƒ"],
        "å¨±ä¹": ["å¨±ä¹", "ç”µå½±", "éŸ³ä¹", "æ˜æ˜Ÿ", "æ¼”å‘˜", "æ­Œæ‰‹", "èŠ‚ç›®", "è¡¨æ¼”", "è‰ºæœ¯", "åª’ä½“"],
        "å›½é™…": ["å›½é™…", "è”åˆå›½", "å…¨çƒ", "ä¸–ç•Œ", "å¤–äº¤", "åˆä½œ", "å³°ä¼š", "åè®®", "ç»„ç»‡", "å¤šè¾¹"],
        "ç¾éš¾": ["ç¾éš¾", "åœ°éœ‡", "æ´ªæ°´", "ç«ç¾", "äº‹æ•…", "æ•‘æ´", "ä¼¤äº¡", "æŸå¤±", "ç´§æ€¥", "å±é™©"],
        "çŠ¯ç½ª": ["çŠ¯ç½ª", "æ¡ˆä»¶", "è­¦å¯Ÿ", "è°ƒæŸ¥", "é€®æ•", "å®¡åˆ¤", "æ³•å¾‹", "æ³•é™¢", "å«Œç–‘", "è¯æ®"]
    }
    
    # è®¡ç®—æ¯ä¸ªä¸»é¢˜çš„åŒ¹é…åº¦
    topic_scores = {}
    for topic, topic_kw in topic_keywords.items():
        score = sum(1 for kw in keywords if kw in topic_kw)
        if score > 0:
            topic_scores[topic] = score
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…çš„ä¸»é¢˜ï¼Œä½¿ç”¨å‰ä¸¤ä¸ªå…³é”®è¯ä½œä¸ºæ ‡ç­¾
    if not topic_scores:
        return f"{keywords[0]}ç›¸å…³" if len(keywords) >= 1 else "æœªåˆ†ç±»"
    
    # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ä¸»é¢˜
    best_topic = max(topic_scores, key=topic_scores.get)
    
    # å¦‚æœæœ‰å¤šä¸ªå…³é”®è¯åŒ¹é…ï¼Œå¯ä»¥æ·»åŠ æ›´å…·ä½“çš„æè¿°
    if topic_scores[best_topic] >= 2:
        return f"{best_topic}ç›¸å…³"
    else:
        # å¦‚æœåªæœ‰ä¸€ä¸ªå…³é”®è¯åŒ¹é…ï¼Œæ·»åŠ è¯¥å…³é”®è¯ä½œä¸ºå…·ä½“æè¿°
        matched_keyword = next((kw for kw in keywords if kw in topic_keywords[best_topic]), None)
        if matched_keyword:
            return f"{best_topic}({matched_keyword})"
        else:
            return f"{best_topic}ç›¸å…³"


# çŸ¥è¯†åº“æ•°æ®èšç±»åˆ†ææŠ¥å‘Šï¼Œå±•ç¤ºå…³é”®å‰10åˆ†å¸ƒ
def knowledge_base_cluster_analysis():
    """å¯¹çŸ¥è¯†åº“ä¸­çš„æ•°æ®è¿›è¡Œèšç±»åˆ†æï¼Œå¹¶å±•ç¤ºå…³é”®å‰10åˆ†å¸ƒ"""
    try:
        app_logger.info("å¼€å§‹çŸ¥è¯†åº“æ•°æ®èšç±»åˆ†æ")
        
        # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹åç§°å’Œè·¯å¾„
        embedding_model_name = os.getenv("EMBEDDING_MODEL_NAME")
        faiss_index_path = os.getenv("FAISS_INDEX_PATH")
        vectorstore_path = os.path.dirname(faiss_index_path)
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model_name,
            model_kwargs={'device': 'cpu'}
        )
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        if os.path.exists(faiss_index_path):
            try:
                vectorstore = FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)
                app_logger.info("æˆåŠŸåŠ è½½å‘é‡æ•°æ®åº“")
            except Exception as e:
                app_logger.error(f"åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥ï¼š{str(e)}")
                return {"error": f"åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥ï¼š{str(e)}"}
        else:
            app_logger.error("å‘é‡æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
            return {"error": "å‘é‡æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨"}
        
        # è·å–å‘é‡æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£
        # ç”±äºFAISSä¸ç›´æ¥æä¾›è·å–æ‰€æœ‰æ–‡æ¡£çš„æ–¹æ³•ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ç´¢å¼•æ¥è·å–
        try:
            # è·å–æ–‡æ¡£å‘é‡
            docstore = vectorstore.docstore
            index_to_docstore_id = vectorstore.index_to_docstore_id
            
            # æå–æ‰€æœ‰æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®
            documents = []
            for idx, doc_id in enumerate(index_to_docstore_id.values()):
                try:
                    doc = docstore.search(doc_id)
                    if doc is not None:
                        # æ£€æŸ¥docæ˜¯Documentå¯¹è±¡è¿˜æ˜¯å­—ç¬¦ä¸²
                        if hasattr(doc, 'page_content'):
                            # è¿™æ˜¯ä¸€ä¸ªDocumentå¯¹è±¡
                            documents.append({
                                "content": doc.page_content,
                                "metadata": doc.metadata if hasattr(doc, 'metadata') else {}
                            })
                        else:
                            # è¿™æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²æˆ–å…¶ä»–å¯¹è±¡
                            documents.append({
                                "content": str(doc),
                                "metadata": {}
                            })
                except (KeyError, AttributeError) as e:
                    # å¦‚æœæ–‡æ¡£IDä¸å­˜åœ¨äºdocstoreä¸­ï¼Œè·³è¿‡è¯¥æ–‡æ¡£
                    continue
            
            app_logger.info(f"ä»å‘é‡æ•°æ®åº“ä¸­æå–äº†{len(documents)}ä¸ªæ–‡æ¡£")
            
            if len(documents) < 10:
                app_logger.warning("æ–‡æ¡£æ•°é‡ä¸è¶³10ä¸ªï¼Œèšç±»åˆ†æç»“æœå¯èƒ½ä¸å‡†ç¡®")
                
            # æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
            def preprocess_text(text, tags=None):
                import re
                from bs4 import BeautifulSoup
                
                # å»é™¤HTMLæ ‡ç­¾
                if '<' in text and '>' in text:
                    soup = BeautifulSoup(text, 'html.parser')
                    text = soup.get_text()
                
                # å»é™¤URL
                text = re.sub(r'https?://\S+|www\.\S+', '', text)
                
                # å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—ï¼Œä¿ç•™ä¸­è‹±æ–‡
                text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z\s]', ' ', text)

                # é¢å¤–ç§»é™¤å¸¸è§HTMLå±æ€§/æ— æ„ä¹‰æ ‡è®°åŠå…¶ç»„åˆï¼ˆå¦‚ dirã€ltrã€fr ä»¥åŠåŒ…å«å®ƒä»¬çš„çŸ­è¯­ï¼‰
                banned_tokens = [
                    'dir', 'ltr', 'rtl', 'align', 'center', 'left', 'right', 'nbsp', 'nbspnbsp',
                    'figcaption', 'caption', 'blockquote', 'figure', 'video', 'pagevideo', 'poster', 'style'
                ]
                # ç§»é™¤ç‹¬ç«‹å‡ºç°çš„ç¦ç”¨è¯
                pattern_single = r'\b(' + '|'.join(banned_tokens) + r')\b'
                text = re.sub(pattern_single, ' ', text, flags=re.IGNORECASE)
                # ç§»é™¤å«ç¦ç”¨è¯çš„äºŒå…ƒçŸ­è¯­ï¼ˆä¾‹ï¼š"dir ltr", "ebmt dir", "bbc fr"ï¼‰
                pattern_bigram = r'\b(?:\w+\s+)?(' + '|'.join(banned_tokens) + r')(?:\s+\w+)?\b'
                text = re.sub(pattern_bigram, ' ', text, flags=re.IGNORECASE)
                
                # å¤„ç†tagså­—æ®µ
                if tags and isinstance(tags, str):
                    # å°†tagsæŒ‰é€—å·åˆ†å‰²ï¼Œå¹¶è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²
                    tag_words = [tag.strip() for tag in tags.split(',') if tag.strip()]
                    # å°†tagsæ·»åŠ åˆ°æ–‡æœ¬ä¸­ï¼Œå¢åŠ æƒé‡ï¼ˆé‡å¤3æ¬¡ï¼‰
                    if tag_words:
                        text = text + ' ' + ' '.join(tag_words * 3)
                
                # å»é™¤å¤šä½™ç©ºæ ¼
                text = re.sub(r'\s+', ' ', text).strip()
                
                return text
            
            # é¢„å¤„ç†æ‰€æœ‰æ–‡æ¡£å†…å®¹ï¼ŒåŒ…æ‹¬tagså­—æ®µ
            processed_contents = []
            for doc in documents:
                # è·å–tagså­—æ®µï¼Œå¦‚æœmetadataä¸­ä¸å­˜åœ¨åˆ™ä¸ºç©ºå­—ç¬¦ä¸²
                tags = doc.get("metadata", {}).get("tags", "")
                # é¢„å¤„ç†æ–‡æœ¬ï¼ŒåŒ…æ‹¬tags
                processed_content = preprocess_text(doc["content"], tags)
                processed_contents.append(processed_content)
            
            # ä½¿ç”¨TF-IDFå‘é‡åŒ–æ–‡æ¡£ï¼Œæ”¯æŒä¸­è‹±æ–‡
            # æ·»åŠ è‡ªå®šä¹‰åœç”¨è¯
            custom_stop_words = [
                'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'ä½œè€…', 'ç¬”è€…', 'æ—¥ç”µ', 'å¥¹è¯´', 'å¦æœ‰', 'ä»Šå¹´', 'å»å¹´', 'æ˜å¹´', 'æ—©äº›',
                'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'am', 'http', 'https', 'www', 'com', 'cn', 'org', 'net', 'href', 'target', 'blank', 'src', 'img', 'figure', 'figcaption', 'div', 'class', 'style', 'id', 'title', 'alt', 'author', 'margin', 'auto', 'display', 'block', 'float', 'none', 'width', 'height', 'padding', 'border', 'background', 'color', 'font', 'text', 'align', 'center', 'left', 'right', 'top', 'bottom', 'position', 'relative', 'absolute', 'fixed', 'clear', 'overflow', 'hidden', 'visible', 'scroll', 'z-index', 'opacity', 'transition', 'transform', 'box-shadow', 'border-radius', 'outline', 'cursor', 'pointer', 'box', 'index', 'radius', 'shadow',
                'as', 'it', 'than', 'said', 'say', 'new', 'old', 'up', 'more', 'he', 'she', 'him', 'her', 'me', 'my', 'mine'
            ]
            
            vectorizer = TfidfVectorizer(
                max_features=2000,  # å¢åŠ ç‰¹å¾æ•°é‡
                stop_words=custom_stop_words,
                ngram_range=(1, 2),  # æ·»åŠ äºŒå…ƒè¯­æ³•
                min_df=2,  # æœ€å°æ–‡æ¡£é¢‘ç‡
                max_df=0.8,  # æœ€å¤§æ–‡æ¡£é¢‘ç‡
                sublinear_tf=True  # ä½¿ç”¨å­çº¿æ€§TFç¼©æ”¾
            )
            tfidf_matrix = vectorizer.fit_transform(processed_contents)
            
            # ä½¿ç”¨UMAPé™ç»´å’ŒHDBSCANèšç±»ï¼ˆæ›´å…ˆè¿›çš„èšç±»æ–¹æ³•ï¼‰
            try:
                # å°è¯•å¯¼å…¥UMAPå’ŒHDBSCAN
                import umap
                import hdbscan
                use_advanced_clustering = True
                app_logger.info("ä½¿ç”¨UMAPé™ç»´å’ŒHDBSCANèšç±»")
            except ImportError:
                use_advanced_clustering = False
                app_logger.warning("æœªå®‰è£…UMAPæˆ–HDBSCANï¼Œä½¿ç”¨ä¼ ç»ŸK-meansèšç±»")
            
            if use_advanced_clustering:
                # ä½¿ç”¨UMAPè¿›è¡Œé™ç»´
                # ä¼˜åŒ–UMAPå‚æ•°ï¼Œæé«˜é™ç»´è´¨é‡
                n_neighbors = min(15, max(5, int(len(documents) * 0.3)))
                if n_neighbors < 2:
                    n_neighbors = 2
                    
                n_components = min(50, len(documents) - 1, len(documents) // 2)
                if n_components < 2:
                    n_components = 2
                    
                # ä¼˜åŒ–UMAPå‚æ•°é…ç½®
                umap_params = {
                    'n_components': n_components,  # é™ç»´åçš„ç»´åº¦
                    'n_neighbors': n_neighbors,  # é‚»å±…æ•°é‡
                    'min_dist': 0.05,  # å‡å°æœ€å°è·ç¦»ï¼Œä½¿èšç±»æ›´ç´§å‡‘
                    'metric': 'cosine',  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ›´é€‚åˆæ–‡æœ¬æ•°æ®
                    'random_state': 42,
                    'transform_seed': 42,
                    'spread': 1.0,  # å¢åŠ æ‰©å±•å‚æ•°ï¼Œä½¿ç‚¹åˆ†å¸ƒæ›´å‡åŒ€
                    'local_connectivity': 2,  # å¢åŠ å±€éƒ¨è¿é€šæ€§
                    'repulsion_strength': 1.0,  # å¢åŠ æ’æ–¥å¼ºåº¦
                    'negative_sample_rate': 5,  # å¢åŠ è´Ÿé‡‡æ ·ç‡
                }
                
                reducer = umap.UMAP(**umap_params)
                
                # å¦‚æœæ•°æ®é›†å¾ˆå°ï¼Œç›´æ¥ä½¿ç”¨TF-IDFçŸ©é˜µè€Œä¸é™ç»´
                if len(documents) <= 10:
                    app_logger.info("æ•°æ®é›†å¤ªå°ï¼Œè·³è¿‡UMAPé™ç»´ï¼Œç›´æ¥ä½¿ç”¨TF-IDFçŸ©é˜µ")
                    embedding = tfidf_matrix.toarray()
                else:
                    try:
                        embedding = reducer.fit_transform(tfidf_matrix)
                        app_logger.info(f"UMAPé™ç»´å®Œæˆï¼ŒåŸå§‹ç»´åº¦: {tfidf_matrix.shape[1]}ï¼Œé™ç»´åç»´åº¦: {embedding.shape[1]}")
                        
                        # å¯¹é™ç»´åçš„æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
                        from sklearn.preprocessing import StandardScaler
                        scaler = StandardScaler()
                        embedding = scaler.fit_transform(embedding)
                        
                        app_logger.info("é™ç»´æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")
                    except Exception as e:
                        app_logger.warning(f"UMAPé™ç»´å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨TF-IDFçŸ©é˜µä½œä¸ºæ›¿ä»£")
                        embedding = tfidf_matrix.toarray()
            
            if use_advanced_clustering:
                # ä½¿ç”¨HDBSCANè¿›è¡Œèšç±»
                # é™ä½å‚æ•°è¦æ±‚ï¼Œä½¿èšç±»æ›´å®¹æ˜“å½¢æˆ
                min_cluster_size = max(2, int(len(documents) * 0.03))  # é™ä½æœ€å°èšç±»å¤§å°
                min_samples = max(1, int(len(documents) * 0.01))  # é™ä½æœ€å°æ ·æœ¬æ•°
                
                # å°è¯•ä¸åŒçš„èšç±»é€‰æ‹©æ–¹æ³•ï¼Œæ‰¾åˆ°æœ€ä½³ç»“æœ
                best_cluster_labels = None
                best_optimal_clusters = 0
                best_silhouette_avg = -1
                
                for cluster_selection in ['eom', 'leaf']:
                    try:
                        clusterer = hdbscan.HDBSCAN(
                            min_cluster_size=min_cluster_size,
                            min_samples=min_samples,
                            metric='euclidean',
                            cluster_selection_method=cluster_selection,
                            prediction_data=True,
                            gen_min_span_tree=True  # ç”Ÿæˆæœ€å°ç”Ÿæˆæ ‘ä»¥æé«˜èšç±»è´¨é‡
                        )
                        cluster_labels = clusterer.fit_predict(embedding)
                        
                        # ç»Ÿè®¡èšç±»æ•°é‡ï¼ˆå¿½ç•¥å™ªå£°ç‚¹ï¼Œæ ‡ç­¾ä¸º-1ï¼‰
                        unique_labels = set(cluster_labels)
                        if -1 in unique_labels:
                            unique_labels.remove(-1)
                        current_optimal_clusters = len(unique_labels)
                        
                        # è®¡ç®—è½®å»“ç³»æ•°ï¼ˆæ’é™¤å™ªå£°ç‚¹ï¼‰
                        if current_optimal_clusters > 1:
                            non_noise_indices = [i for i, label in enumerate(cluster_labels) if label != -1]
                            if len(non_noise_indices) > current_optimal_clusters:
                                try:
                                    current_silhouette_avg = silhouette_score(
                                        embedding[non_noise_indices], 
                                        cluster_labels[non_noise_indices]
                                    )
                                    
                                    # å¦‚æœå½“å‰èšç±»ç»“æœæ›´å¥½ï¼Œåˆ™ä¿å­˜
                                    if current_silhouette_avg > best_silhouette_avg:
                                        best_silhouette_avg = current_silhouette_avg
                                        best_optimal_clusters = current_optimal_clusters
                                        best_cluster_labels = cluster_labels.copy()
                                except Exception as e:
                                    app_logger.warning(f"è®¡ç®—è½®å»“ç³»æ•°å¤±è´¥: {str(e)}")
                                    continue
                    except Exception as e:
                        app_logger.warning(f"èšç±»æ–¹æ³• {cluster_selection} å¤±è´¥: {str(e)}")
                        continue
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„èšç±»ç»“æœï¼Œä½¿ç”¨é»˜è®¤æ–¹æ³•
                if best_cluster_labels is None:
                    clusterer = hdbscan.HDBSCAN(
                        min_cluster_size=min_cluster_size,
                        min_samples=min_samples,
                        metric='euclidean',
                        cluster_selection_method='eom',
                        prediction_data=True
                    )
                    best_cluster_labels = clusterer.fit_predict(embedding)
                    
                    # ç»Ÿè®¡èšç±»æ•°é‡
                    unique_labels = set(best_cluster_labels)
                    if -1 in unique_labels:
                        unique_labels.remove(-1)
                    best_optimal_clusters = len(unique_labels)
                    best_silhouette_avg = 0
                
                cluster_labels = best_cluster_labels
                optimal_clusters = best_optimal_clusters
                silhouette_avg = best_silhouette_avg
                
                app_logger.info(f"HDBSCANèšç±»å®Œæˆï¼Œå‘ç°{optimal_clusters}ä¸ªèšç±»ï¼Œè½®å»“ç³»æ•°: {silhouette_avg:.4f}")
                
                # å¦‚æœèšç±»æ•°é‡è¿‡å¤šï¼Œå°è¯•åˆå¹¶å°èšç±»
                if optimal_clusters > 60:
                    app_logger.info("èšç±»æ•°é‡è¿‡å¤šï¼Œå°è¯•åˆå¹¶å°èšç±»")
                    # ç»Ÿè®¡æ¯ä¸ªèšç±»çš„æ–‡æ¡£æ•°é‡
                    cluster_counts = Counter(cluster_labels)
                    # æ‰¾å‡ºæ–‡æ¡£æ•°é‡æœ€å°‘çš„èšç±»
                    small_clusters = [cid for cid, count in cluster_counts.items() 
                                     if cid != -1 and count < max(1, len(documents) * 0.02)]
                    
                    if small_clusters:
                        # å°†å°èšç±»åˆå¹¶åˆ°æœ€è¿‘çš„èšç±»
                        for small_cluster_id in small_clusters:
                            # æ‰¾åˆ°å°èšç±»çš„ä¸­å¿ƒç‚¹
                            small_cluster_indices = [i for i, label in enumerate(cluster_labels) if label == small_cluster_id]
                            if small_cluster_indices:
                                small_cluster_center = embedding[small_cluster_indices].mean(axis=0)
                                
                                # æ‰¾åˆ°æœ€è¿‘çš„èšç±»
                                min_distance = float('inf')
                                nearest_cluster = -1
                                
                                for cluster_id in set(cluster_labels):
                                    if cluster_id != -1 and cluster_id != small_cluster_id:
                                        cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]
                                        if cluster_indices:
                                            cluster_center = embedding[cluster_indices].mean(axis=0)
                                            distance = np.linalg.norm(small_cluster_center - cluster_center)
                                            if distance < min_distance:
                                                min_distance = distance
                                                nearest_cluster = cluster_id
                                
                                # åˆå¹¶èšç±»
                                if nearest_cluster != -1:
                                    for idx in small_cluster_indices:
                                        cluster_labels[idx] = nearest_cluster
                                    app_logger.info(f"å°†èšç±»{small_cluster_id}åˆå¹¶åˆ°èšç±»{nearest_cluster}")
                        
                        # é‡æ–°ç»Ÿè®¡èšç±»æ•°é‡
                        unique_labels = set(cluster_labels)
                        if -1 in unique_labels:
                            unique_labels.remove(-1)
                        optimal_clusters = len(unique_labels)
                        
                        # é‡æ–°è®¡ç®—è½®å»“ç³»æ•°
                        if optimal_clusters > 1:
                            non_noise_indices = [i for i, label in enumerate(cluster_labels) if label != -1]
                            if len(non_noise_indices) > optimal_clusters:
                                try:
                                    silhouette_avg = silhouette_score(
                                        embedding[non_noise_indices], 
                                        cluster_labels[non_noise_indices]
                                    )
                                except Exception as e:
                                    app_logger.warning(f"é‡æ–°è®¡ç®—è½®å»“ç³»æ•°å¤±è´¥: {str(e)}")
                                    silhouette_avg = best_silhouette_avg
                        
                        app_logger.info(f"åˆå¹¶åèšç±»æ•°é‡: {optimal_clusters}ï¼Œè½®å»“ç³»æ•°: {silhouette_avg:.4f}")
                
            else:
                # ä½¿ç”¨ä¼ ç»ŸK-meansèšç±»
                max_possible_clusters = min(15, len(documents) // 2) if len(documents) > 20 else 10
                if max_possible_clusters < 2:
                    max_possible_clusters = 2
                
                # è®¡ç®—ä¸åŒèšç±»æ•°é‡çš„è½®å»“ç³»æ•°
                silhouette_scores = []
                possible_clusters = range(2, min(max_possible_clusters + 1, 11))
                
                for n_clusters in possible_clusters:
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                    cluster_labels = kmeans.fit_predict(tfidf_matrix)
                    silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels)
                    silhouette_scores.append(silhouette_avg)
                
                # é€‰æ‹©è½®å»“ç³»æ•°æœ€å¤§çš„èšç±»æ•°é‡
                optimal_clusters = possible_clusters[silhouette_scores.index(max(silhouette_scores))]
                silhouette_avg = max(silhouette_scores)
                
                # ä½¿ç”¨K-meansèšç±»
                kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(tfidf_matrix)
                
                app_logger.info(f"K-meansèšç±»å®Œæˆï¼Œèšç±»æ•°é‡: {optimal_clusters}ï¼Œè½®å»“ç³»æ•°: {silhouette_avg:.4f}")
            
            # è·å–èšç±»æ ‡ç­¾
            labels = cluster_labels
            
            # ç»Ÿè®¡æ¯ä¸ªèšç±»çš„æ–‡æ¡£æ•°é‡
            cluster_counts = Counter(labels)
            
            # å¦‚æœä½¿ç”¨HDBSCANï¼Œç»Ÿè®¡å™ªå£°ç‚¹æ•°é‡
            if use_advanced_clustering and -1 in cluster_counts:
                noise_count = cluster_counts[-1]
                app_logger.info(f"å‘ç°{noise_count}ä¸ªå™ªå£°ç‚¹ï¼ˆæœªå½’ç±»åˆ°ä»»ä½•èšç±»çš„æ–‡æ¡£ï¼‰")
                # ä»èšç±»ç»Ÿè®¡ä¸­ç§»é™¤å™ªå£°ç‚¹
                del cluster_counts[-1]
            
            # æå–æ¯ä¸ªèšç±»çš„å…³é”®è¯ - æ”¹è¿›æ–¹æ³•
            feature_names = vectorizer.get_feature_names_out()
            cluster_keywords = {}
            
            # è¿‡æ»¤æ— æ„ä¹‰è¯æ±‡çš„é€šç”¨æ–¹æ³•
            def is_meaningful_word(word):
                # 1. åŸºæœ¬é•¿åº¦è¿‡æ»¤
                if len(word) < 2:
                    return False
                
                # 2. è¿‡æ»¤å•å­—è¯æ±‡ï¼ˆé™¤éæ˜¯ç‰¹å®šæœ‰æ„ä¹‰çš„å•å­—ï¼‰
                if len(word) == 1 and word not in ['æ°´', 'ç«', 'åœŸ', 'é‡‘', 'æœ¨', 'è½¦', 'è·¯', 'æ¡¥', 'è¯', 'ç—…', 'æ³•', 'ç¨', 'æ”¿', 'å†›', 'å›½']:
                    return False

                # 2.1 ä»»ä½•åŒ…å«ä»¥ä¸‹å­ä¸²çš„è¯ï¼ˆåŒ…æ‹¬äºŒå…ƒçŸ­è¯­ï¼‰ç›´æ¥è¿‡æ»¤
                banned_substrings = [
                    'dir', 'ltr', 'rtl', 'align', 'center', 'left', 'right', 'nbsp',
                    'figcaption', 'caption', 'blockquote', 'figure', 'video', 'pagevideo', 'poster', 'style'
                ]
                lower_word = word.lower()
                if any(sub in lower_word for sub in banned_substrings):
                    return False
                
                # 3. è¿‡æ»¤è¿‡é•¿è¯æ±‡ï¼ˆè¶…è¿‡15ä¸ªå­—ç¬¦çš„è¯æ±‡é€šå¸¸ä¸æ˜¯å¥½çš„å…³é”®è¯ï¼‰
                if len(word) > 15:
                    return False
                
                # 4. è¿‡æ»¤å¸¸è§æ— æ„ä¹‰åç¼€
                meaningless_suffixes = ['çš„', 'äº†', 'ç€', 'è¿‡', 'ä»¬', 'ä¸­', 'å†…', 'å¤–', 'ä¸Š', 'ä¸‹', 'å·¦', 'å³', 'å‰', 'å', 'é‡Œ', 'é—´', 'ä¾§', 'è¾¹', 'é¢', 'å¤´', 'å°¾', 'éƒ¨', 'åˆ†', 'æœˆ', 'å¹´', 'æ—¥', 'æ—¶', 'åˆ†', 'ç§’', 'åº¦', 'ç±³', 'å…‹', 'å¨', 'ä¸ª', 'åª', 'æ¡', 'å¼ ', 'ä»¶', 'ç§', 'ç±»', 'å‹', 'å¼', 'æ ·', 'ç­‰', 'çº§', 'æ•°', 'é‡', 'æ¯”', 'ç‡', 'å€¼', 'ä»·', 'é’±', 'å…ƒ', 'å—', 'æ¯›']
                if word.endswith(tuple(meaningless_suffixes)):
                    return False
                
                # 5. è¿‡æ»¤å¸¸è§æ— æ„ä¹‰å‰ç¼€
                meaningless_prefixes = ['é', 'æ— ', 'ä¸', 'æ²¡', 'æœª', 'åˆ«', 'è«', 'å‹¿']
                if word.startswith(tuple(meaningless_prefixes)) and len(word) <= 3:
                    return False
                
                # 6. è¿‡æ»¤çº¯æ•°å­—æˆ–ä»¥æ•°å­—å¼€å¤´çš„è¯
                if word.isdigit() or (len(word) > 1 and word[0].isdigit()):
                    return False
                
                # 7. è¿‡æ»¤å¸¸è§æ— æ„ä¹‰ç»„åˆè¯
                meaningless_combinations = [
                    'åˆ†å·¦å³', 'åˆ†ä¸Šä¸‹', 'åˆ†å‰å', 'åˆ†å†…å¤–', 'åˆ†ä¸œè¥¿', 'åˆ†å—åŒ—', 'åˆ†å¤§å°', 'åˆ†å¤šå°‘', 'åˆ†é«˜ä½', 'åˆ†é•¿çŸ­',
                    'çš„å·¦å³', 'çš„ä¸Šä¸‹', 'çš„å‰å', 'çš„å†…å¤–', 'çš„ä¸­é—´', 'çš„æ—è¾¹', 'çš„é™„è¿‘', 'çš„å‘¨å›´', 'çš„ä¸Šé¢', 'çš„ä¸‹é¢',
                    'æœˆå·¦å³', 'å¹´å·¦å³', 'æ—¥å·¦å³', 'æ—¶å·¦å³', 'åˆ†å·¦å³', 'ç§’å·¦å³', 'åº¦å·¦å³', 'ç±³å·¦å³', 'å…‹å·¦å³', 'å¨å·¦å³',
                    'åˆ†å·¦å³', 'åˆ†ä¸Šä¸‹', 'åˆ†å‰å', 'åˆ†å†…å¤–', 'åˆ†ä¸œè¥¿', 'åˆ†å—åŒ—', 'åˆ†å¤§å°', 'åˆ†å¤šå°‘', 'åˆ†é«˜ä½', 'åˆ†é•¿çŸ­',
                    'çš„æœˆ', 'çš„å¹´', 'çš„æ—¥', 'çš„æ—¶', 'çš„åˆ†', 'çš„ç§’', 'çš„åº¦', 'çš„ç±³', 'çš„å…‹', 'çš„å¨', 'çš„ä¸ª', 'çš„åª', 'çš„æ¡', 'çš„å¼ ',
                    'æœˆçš„', 'å¹´çš„', 'æ—¥çš„', 'æ—¶çš„', 'åˆ†çš„', 'ç§’çš„', 'åº¦çš„', 'ç±³çš„', 'å…‹çš„', 'å¨çš„', 'ä¸ªçš„', 'åªçš„', 'æ¡çš„', 'å¼ çš„',
                    # æ–°å¢çš„æ— æ„ä¹‰ç»„åˆ
                    'æ—¥è‡³', 'æ—¥è¯´', 'å¹¶äº', 'æŠ¥é“', 'ä»–è¯´', 'æ—¥è‡³', 'æ—¥æŠ¥é“', 'æ—¥è¯´', 'è·¯é€ç¤¾æŠ¥é“', 'æ³•æ–°ç¤¾æŠ¥é“', 'å½­åšç¤¾æŠ¥é“', 'æ–°åç¤¾æŠ¥é“'
                ]
                if word in meaningless_combinations:
                    return False
                
                # 8. è¿‡æ»¤é‡å¤å­—ç¬¦è¯ï¼ˆå¦‚"å•Šå•Šå•Š"ã€"å“ˆå“ˆå“ˆ"ç­‰ï¼‰
                if len(set(word)) == 1 and len(word) > 2:
                    return False
                
                # 9. è¿‡æ»¤HTML/XMLç›¸å…³è¯æ±‡
                html_terms = ['href', 'src', 'img', 'div', 'class', 'style', 'id', 'title', 'alt', 'http', 'https', 'www', 'com', 'cn', 'org', 'net', 'target', 'blank', '_blank']
                if word in html_terms:
                    return False
                
                # 10. è¿‡æ»¤ç½‘ç«™ç›¸å…³è¯æ±‡
                website_terms = ['zaobao', 'sg', 'cassette', 'sphdigital', 'image', 'figcapton']
                if word in website_terms:
                    return False
                
                # 11. è¿‡æ»¤å¸¸è§åª’ä½“åç§°ï¼ˆé™¤éå®ƒä»¬æ˜¯å…³é”®è¯çš„æ ¸å¿ƒéƒ¨åˆ†ï¼‰
                media_names = ['è·¯é€ç¤¾', 'æ³•æ–°ç¤¾', 'å½­åšç¤¾', 'æ–°åç¤¾', 'ç¾è”ç¤¾', 'å¡”æ–¯ç¤¾', 'å…±åŒç¤¾']
                if word in media_names and len(word) <= 4:
                    return False
                
                # 12. è¿‡æ»¤æ—¶é—´ç›¸å…³è¯æ±‡
                time_patterns = [
                    r'\d{1,2}æœˆ\d{1,2}æ—¥', r'\d{4}å¹´\d{1,2}æœˆ', r'\d{1,2}æ—¥', r'\d{1,2}æœˆ', r'\d{4}å¹´',
                    r'æ˜ŸæœŸ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒæ—¥]', r'å‘¨[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒæ—¥]', r'ç¤¼æ‹œ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒæ—¥]'
                ]
                import re
                for pattern in time_patterns:
                    if re.match(pattern, word):
                        return False
                
                # 13. è¿‡æ»¤è¿æ¥è¯å’Œè¿‡æ¸¡è¯
                transition_words = ['ç„¶è€Œ', 'ä½†æ˜¯', 'ä¸è¿‡', 'å› æ­¤', 'æ‰€ä»¥', 'äºæ˜¯', 'å¦å¤–', 'æ­¤å¤–', 'åŒæ—¶', 'å…¶æ¬¡', 'é¦–å…ˆ', 'æœ€å', 'æ€»ä¹‹', 'ç»¼ä¸Š', 'å¦å¤–', 'æ­¤å¤–']
                if word in transition_words:
                    return False
                
                # 14. è¿‡æ»¤å¸¸è§æŠ¥é“ç”¨è¯­
                reporting_phrases = ['æ®äº†è§£', 'æ®æŠ¥é“', 'æ®ä»‹ç»', 'æ®æ‚‰', 'æ®ç§°', 'æ®é€éœ²', 'æ®æ¶ˆæ¯', 'æ®è§‚å¯Ÿ', 'æ®åˆ†æ', 'æ®ç»Ÿè®¡']
                if word in reporting_phrases:
                    return False
                
                # 15. è¿‡æ»¤çŸ­æ—¶é—´ç›¸å…³è¯æ±‡
                short_time_phrases = ['å¤©å†…æ¢å¤', 'æœˆå†…å®Œæˆ', 'å¹´å†…å®ç°', 'å‘¨å†…è§£å†³', 'æ—¥å‰', 'æ—¥å', 'æœˆå‰', 'å¹´å']
                if word in short_time_phrases:
                    return False
                
                # 16. è¿‡æ»¤æ¡ä»¶è¿è¯
                condition_words = ['è‹¥', 'å¦‚æœ', 'å‡å¦‚', 'å‡ä½¿', 'å€˜è‹¥', 'è¦æ˜¯', 'ä¸‡ä¸€', 'åªè¦', 'åªæœ‰', 'é™¤é', 'ä¸€æ—¦', 'å¦‚æœ', 'è‹¥æ˜¯', 'è‹¥æœ', 'è‹¥ä½¿', 'è‹¥ä¸º']
                if word in condition_words:
                    return False
                
                # 17. è¿‡æ»¤ä»¥æ¡ä»¶è¿è¯å¼€å¤´çš„çŸ­è¯­
                condition_prefixes = ['è‹¥', 'å¦‚æœ', 'å‡å¦‚', 'å‡ä½¿', 'å€˜è‹¥', 'è¦æ˜¯', 'ä¸‡ä¸€', 'åªè¦', 'åªæœ‰', 'é™¤é', 'ä¸€æ—¦']
                if any(word.startswith(prefix) for prefix in condition_prefixes):
                    return False
                
                return True
            
            for cluster_id in sorted(cluster_counts.keys()):
                # è·å–è¯¥èšç±»çš„æ–‡æ¡£ç´¢å¼•
                cluster_indices = [idx for idx, label in enumerate(labels) if label == cluster_id]
                
                if not cluster_indices:
                    continue
                
                # è®¡ç®—è¯¥èšç±»ä¸­æ¯ä¸ªè¯çš„å¹³å‡TF-IDFå€¼
                cluster_tfidf_mean = tfidf_matrix[cluster_indices].mean(axis=0)
                cluster_tfidf_mean = np.array(cluster_tfidf_mean).flatten()
                
                # æå–èšç±»å†…æ‰€æœ‰æ–‡æ¡£çš„æ ‡é¢˜ï¼Œå¹¶å¢å¼ºæ ‡é¢˜ä¸­è¯æ±‡çš„æƒé‡
                cluster_titles = [documents[i]["metadata"]["title"] for i in cluster_indices if "title" in documents[i]["metadata"]]
                if cluster_titles:
                    # å°†æ‰€æœ‰æ ‡é¢˜åˆå¹¶ä¸ºä¸€ä¸ªæ–‡æœ¬
                    titles_text = ' '.join(cluster_titles)
                    # åˆ†è¯å¹¶ç»Ÿè®¡æ ‡é¢˜ä¸­çš„è¯é¢‘
                    title_words = re.findall(r'\b\w+\b', titles_text.lower())
                    title_word_freq = Counter(title_words)
                    
                    # å¢å¼ºæ ‡é¢˜ä¸­è¯æ±‡çš„TF-IDFæƒé‡ï¼ˆå¢åŠ 50%çš„æƒé‡ï¼‰
                    for word, freq in title_word_freq.items():
                        # æ‰¾åˆ°è¯¥è¯åœ¨feature_namesä¸­çš„ç´¢å¼•
                        word_indices = [i for i, feature in enumerate(feature_names) if feature == word]
                        for idx in word_indices:
                            if idx < len(cluster_tfidf_mean):
                                cluster_tfidf_mean[idx] *= 1.5
                
                # è·å–TF-IDFå€¼æœ€é«˜çš„å‰10ä¸ªå…³é”®è¯
                top_indices = cluster_tfidf_mean.argsort()[-10:][::-1]
                top_keywords = [feature_names[idx] for idx in top_indices if cluster_tfidf_mean[idx] > 0]
                
                # å¦‚æœä½¿ç”¨HDBSCANï¼Œè¿˜å¯ä»¥è€ƒè™‘èšç±»åœ¨é™ç»´ç©ºé—´ä¸­çš„å¯†åº¦
                if use_advanced_clustering:
                    # è®¡ç®—èšç±»åœ¨é™ç»´ç©ºé—´ä¸­çš„ä¸­å¿ƒç‚¹
                    if 'embedding' in locals():
                        cluster_center = embedding[cluster_indices].mean(axis=0)
                        # æ‰¾åˆ°è·ç¦»ä¸­å¿ƒç‚¹æœ€è¿‘çš„æ–‡æ¡£ä½œä¸ºä»£è¡¨æ–‡æ¡£
                        distances = np.linalg.norm(embedding[cluster_indices] - cluster_center, axis=1)
                        representative_doc_idx = cluster_indices[np.argmin(distances)]
                        # ä»ä»£è¡¨æ–‡æ¡£ä¸­æå–é¢å¤–å…³é”®è¯
                        rep_doc_tfidf = tfidf_matrix[representative_doc_idx].toarray().flatten()
                        rep_top_indices = rep_doc_tfidf.argsort()[-5:][::-1]
                        rep_keywords = [feature_names[idx] for idx in rep_top_indices if rep_doc_tfidf[idx] > 0]
                        # æ·»åŠ åˆ°å…³é”®è¯åˆ—è¡¨ï¼ˆå¦‚æœå°šæœªå­˜åœ¨ï¼‰
                        for keyword in rep_keywords:
                            if keyword not in top_keywords and len(top_keywords) < 10:
                                top_keywords.append(keyword)
                
                # å¦‚æœå…³é”®è¯ä¸è¶³10ä¸ªï¼Œå°è¯•ä»èšç±»ä¸­å¿ƒè·å–ï¼ˆä»…é€‚ç”¨äºK-meansï¼‰
                if len(top_keywords) < 10 and not use_advanced_clustering:
                    cluster_center = kmeans.cluster_centers_[cluster_id]
                    additional_indices = cluster_center.argsort()[-(10-len(top_keywords)):][::-1]
                    additional_keywords = [feature_names[idx] for idx in additional_indices if feature_names[idx] not in top_keywords]
                    top_keywords.extend(additional_keywords)
                
                # å¦‚æœä»ç„¶ä¸è¶³10ä¸ªï¼Œä½¿ç”¨èšç±»å†…æ‰€æœ‰æ–‡æ¡£çš„é«˜é¢‘è¯è¡¥å……
                if len(top_keywords) < 10:
                    # ç»Ÿè®¡èšç±»å†…æ‰€æœ‰æ–‡æ¡£çš„è¯é¢‘ï¼ŒåŒ…æ‹¬å†…å®¹å’Œæ ‡é¢˜
                    cluster_contents = [documents[i]["content"] for i in cluster_indices]
                    cluster_titles = [documents[i]["metadata"]["title"] for i in cluster_indices if "title" in documents[i]["metadata"]]
                    
                    # å°†æ ‡é¢˜å’Œå†…å®¹åˆå¹¶ï¼Œä½†ç»™æ ‡é¢˜æ›´é«˜çš„æƒé‡ï¼ˆé‡å¤3æ¬¡ï¼‰
                    cluster_text = ' '.join(cluster_titles * 3 + cluster_contents)
                    
                    # åˆ†è¯å¹¶ç»Ÿè®¡è¯é¢‘
                    words = re.findall(r'\b\w+\b', cluster_text.lower())
                    word_freq = Counter(words)
                    
                    # è¿‡æ»¤æ‰åœç”¨è¯å’Œå·²æœ‰å…³é”®è¯
                    custom_stop_words = [
                        'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'ä½œè€…',
                        'ç¬”è€…',
                        'dir','ltr','fr','the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'am', 'http', 'https', 'www', 'com', 'cn', 'org', 'net', 'href', 'target', 'blank', 'src', 'img', 'figure', 'figcaption', 'div', 'class', 'style', 'id', 'title', 'alt', 'author',
                        'as', 'it', 'than', 'said', 'say', 'new', 'old', 'up', 'more', 'he', 'she', 'him', 'her', 'me', 'my', 'mine'
                    ]
                    
                    filtered_words = [(word, freq) for word, freq in word_freq.most_common() 
                                     if word not in custom_stop_words and word not in top_keywords and is_meaningful_word(word)]
                    
                    # è¡¥å……å…³é”®è¯
                    for word, freq in filtered_words:
                        top_keywords.append(word)
                        if len(top_keywords) >= 10:
                            break
                
                # æœ€ç»ˆè¿‡æ»¤ï¼šå¯¹å·²é€‰å…³é”®è¯è¿›è¡Œæœ€åä¸€æ¬¡æ— æ„ä¹‰è¯è¿‡æ»¤
                top_keywords = [word for word in top_keywords if is_meaningful_word(word)]
                
                cluster_keywords[cluster_id] = top_keywords[:10]  # ç¡®ä¿æœ€å¤š10ä¸ªå…³é”®è¯
                
                # ç«‹å³æ‰“å°å½“å‰èšç±»çš„å…³é”®è¯ï¼Œç”¨äºè¯„ä¼°è¿‡æ»¤æ•ˆæœ
                print(f"èšç±» {cluster_id}: {top_keywords[:10]}")
            
            # æŒ‰èšç±»å¤§å°æ’åº
            sorted_clusters = sorted(cluster_counts.items(), key=lambda x: x[1], reverse=True)
            
            # å‡†å¤‡èšç±»ç»“æœ
            top_clusters = []
            for cluster_id, count in sorted_clusters[:10]:
                # è·å–è¯¥èšç±»çš„ä»£è¡¨æ€§æ–‡æ¡£
                cluster_docs = [documents[i] for i, label in enumerate(labels) if label == cluster_id]
                
                # è®¡ç®—èšç±»åœ¨æ€»æ–‡æ¡£ä¸­çš„å æ¯”
                percentage = (count / len(documents)) * 100
                
                # ä¸ºèšç±»ç”Ÿæˆæè¿°æ€§æ ‡ç­¾
                cluster_label = generate_cluster_label(cluster_keywords.get(cluster_id, []))
                
                top_clusters.append({
                    "cluster_id": int(cluster_id),
                    "cluster_label": cluster_label,
                    "document_count": count,
                    "percentage": round(percentage, 2),
                    "keywords": cluster_keywords.get(cluster_id, []),
                    "sample_documents": [
                        {
                            "content": doc["content"][:200] + "..." if len(doc["content"]) > 200 else doc["content"],
                            "metadata": {
                                "title": doc["metadata"].get("title", ""),
                                "author": doc["metadata"].get("author", ""),
                                "pub_date": doc["metadata"].get("pub_date", ""),
                                "source": doc["metadata"].get("source", ""),
                                "tags": doc["metadata"].get("tags", "")
                            }
                        }
                        for doc in cluster_docs[:3]  # æ¯ä¸ªèšç±»å–å‰3ä¸ªæ–‡æ¡£ä½œä¸ºæ ·æœ¬
                    ]
                })
            
            # å¦‚æœä½¿ç”¨HDBSCANï¼Œæ·»åŠ å™ªå£°ç‚¹ä¿¡æ¯
            noise_info = None
            if use_advanced_clustering and -1 in cluster_labels:
                noise_docs = [documents[i] for i, label in enumerate(labels) if label == -1]
                noise_info = {
                    "count": len(noise_docs),
                    "percentage": round((len(noise_docs) / len(documents)) * 100, 2),
                    "sample_documents": [
                        {
                            "content": doc["content"][:200] + "..." if len(doc["content"]) > 200 else doc["content"],
                            "metadata": {
                                "title": doc["metadata"].get("title", ""),
                                "author": doc["metadata"].get("author", ""),
                                "pub_date": doc["metadata"].get("pub_date", ""),
                                "source": doc["metadata"].get("source", ""),
                                "tags": doc["metadata"].get("tags", "")
                            }
                        }
                        for doc in noise_docs[:3]  # å–å‰3ä¸ªå™ªå£°æ–‡æ¡£ä½œä¸ºæ ·æœ¬
                    ]
                }
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            analysis_report = {
                "total_documents": len(documents),
                "total_clusters": optimal_clusters,
                "analysis_date": str(np.datetime64('now')),
                "top_clusters": top_clusters,
                "cluster_distribution": {
                    str(cluster_id): count for cluster_id, count in sorted_clusters
                },
                "silhouette_score": silhouette_avg,
                "clustering_method": "HDBSCAN with UMAP" if use_advanced_clustering else "K-means",
                "optimization_info": {
                    "original_clusters": max_possible_clusters if not use_advanced_clustering else "N/A",
                    "optimal_clusters": optimal_clusters,
                    "merged_clusters": 0,  # HDBSCANä¸éœ€è¦åˆå¹¶å°èšç±»
                    "noise_points": noise_info["count"] if noise_info else 0
                }
            }
            
            # æ·»åŠ å™ªå£°ç‚¹ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if noise_info:
                analysis_report["noise_info"] = noise_info
            
            # æ‰“å°è¯¦ç»†çš„å…³é”®è¯ä¿¡æ¯ï¼Œç”¨äºè¯„ä¼°è¿‡æ»¤æ•ˆæœ
            print("=== èšç±»å…³é”®è¯è¯¦æƒ… ===")
            for cluster_id, keywords in cluster_keywords.items():
                print(f"èšç±» {cluster_id}: {keywords}")
            
            app_logger.info("çŸ¥è¯†åº“æ•°æ®èšç±»åˆ†æå®Œæˆ")
            return analysis_report
            
        except Exception as e:
            app_logger.error(f"èšç±»åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™ï¼š{str(e)}")
            return {"error": f"èšç±»åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™ï¼š{str(e)}"}
            
    except Exception as e:
        app_logger.error(f"åˆå§‹åŒ–èšç±»åˆ†ææ—¶å‡ºé”™ï¼š{str(e)}")
        return {"error": f"åˆå§‹åŒ–èšç±»åˆ†ææ—¶å‡ºé”™ï¼š{str(e)}"}


if __name__ == "__main__":
    # æµ‹è¯•çŸ¥è¯†åº“å·¥å…·
    try:
        knowledge_base_tool = create_knowledge_base_tool()
        online_search_tool = create_online_search_tool()
        online_search_tool_v2 = create_online_search_tool_v2()
        
        # ä»ç¯å¢ƒå˜é‡è·å–æ•°æ®ç›®å½•è·¯å¾„
        script_dir = os.path.dirname(os.path.abspath(__file__))
        text_path = os.path.join(script_dir, "data/test_input.txt")
        
        def test_online_search():
             # æµ‹è¯•åœ¨çº¿æœç´¢
            app_logger.info("æµ‹è¯•åœ¨çº¿æœç´¢ï¼š")
            try:
                # å°è¯•è¿è¡Œæœç´¢
                result = online_search_tool.invoke("å†å²ä¸Šæœ€å¤§çš„å›½å®¶æ˜¯å“ªä¸ª")
                app_logger.info(f"æœç´¢ç»“æœï¼š{result}")
            except Exception as e:
                app_logger.error(f"æœç´¢æµ‹è¯•å‡ºé”™ï¼š{str(e)}")
                # æ‰“å°å·¥å…·çš„æè¿°ä¿¡æ¯ï¼Œäº†è§£å¦‚ä½•æ­£ç¡®ä½¿ç”¨
            app_logger.info(f"å·¥å…·æè¿°ï¼š{online_search_tool.description}")
        
        def test_online_search_v2():
            # æµ‹è¯•åœ¨çº¿æœç´¢v2
            app_logger.info("æµ‹è¯•åœ¨çº¿æœç´¢v2ï¼š")
            try:
                # å°è¯•è¿è¡Œæœç´¢
                result = online_search_tool_v2.invoke("äººå·¥æ™ºèƒ½æœ€æ–°å‘å±•")
                app_logger.info(f"æœç´¢ç»“æœï¼š{result}")
            except Exception as e:
                app_logger.error(f"æœç´¢æµ‹è¯•å‡ºé”™ï¼š{str(e)}")
                # æ‰“å°å·¥å…·çš„æè¿°ä¿¡æ¯ï¼Œäº†è§£å¦‚ä½•æ­£ç¡®ä½¿ç”¨
            app_logger.info(f"å·¥å…·æè¿°ï¼š{online_search_tool_v2.description}")
        
        def test_store_into_faiss():
            from sqlmodel import Session, select,create_engine
            from models.document import Document
            def get_db_engine():
                db_path = os.getenv("DATABASE_PATH")
                if not db_path:
                    raise ValueError("DATABASE_PATH environment variable is not set")
                return create_engine(
                    f"sqlite:///{db_path}",
                    pool_size=20,
                    max_overflow=30,
                    pool_timeout=60,
                    pool_recycle=3600,
                    pool_pre_ping=True,
                    echo=False
                )
            # æµ‹è¯•å­˜å‚¨æ–‡æ¡£
            # ä» sqliteè·å–æ•°æ®
            engine = get_db_engine()
            with Session(engine) as session:
                documents = session.exec(select(Document)).all()

                # æµ‹è¯•å­˜å‚¨æ–‡æ¡£ - ç›´æ¥ä¼ é€’Documentå¯¹è±¡åˆ—è¡¨
                app_logger.info(knowledge_base_tool.run({"action": "store", "documents": documents}))
            
        def test_retrieve_from_faiss():
            # æµ‹è¯•æ£€ç´¢æ–‡æ¡£
            app_logger.info("æµ‹è¯•æ£€ç´¢æ–‡æ¡£ï¼š")
            app_logger.info(knowledge_base_tool.run({"action": "retrieve", "query": "å¤–å–", "k": 3, "rerank": True}))

        def test_cluster_analysis():
            # æµ‹è¯•èšç±»åˆ†æ
            app_logger.info("æµ‹è¯•èšç±»åˆ†æï¼š")
            app_logger.info(knowledge_base_tool.run({"action": "cluster_analysis"}))

        # test_cluster_analysis()
        # test_store_into_faiss()
        test_retrieve_from_faiss()
        # test_online_search()
    except Exception as e:
        app_logger.error(f"æ‰§è¡Œæµ‹è¯•å‡½æ•°æ—¶å‡ºé”™ï¼š{str(e)}")
        print(f"é”™è¯¯ï¼šæ— æ³•åˆå§‹åŒ–çŸ¥è¯†åº“å·¥å…·ã€‚é”™è¯¯ä¿¡æ¯ï¼š{str(e)}")
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å‹ã€‚")